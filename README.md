
---

# âš¡ Voltix â€” YOLO Multi-Class Object Detection (Hackathon Project)

This repository is developed by **Team Voltix** for the **Hack Of Thrones**.  
It implements a YOLO-based object detection model trained on the **Falcon Duality AI dataset** to identify seven safety-related objects.
 
The model is designed to detect **7 safety-related objects** from images and videos.

---

## ğŸ§  Dataset Overview

**Dataset Name:** Falcon Duality AI  
**Classes (`nc: 7`):**
```
['OxygenTank', 'NitrogenTank', 'FirstAidBox', 'FireAlarm', 'SafetySwitchPanel', 'EmergencyPhone', 'FireExtinguisher']
```

You can get the dataset for training as well as testing dataset on [**Falcon Duality AI**](https://falcon.duality.ai/secure/documentation/7-class-hackathon&utm_source=hackathon&utm_medium=instructions&utm_campaign=hackofthrones)

---

## âš™ï¸ Environment Setup

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/Yousuf-177/Voltix.git
cd Voltix
```

### 2ï¸âƒ£ Create a Virtual Environment (Recommended)
```bash
python -m venv yolovenv
source yolovenv/bin/activate      # On Linux/Mac
yolovenv\Scripts\activate         # On Windows
```

### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

> ğŸ’» **Note:** For detailed instructions on enabling GPU acceleration (NVIDIA, AMD, or Intel), please refer to the [GPU Setup Guide](./GPU_SETUP.md).


---

### Run Model on Custom Test Data
```bash
python yolo_detect.py --model yolov8s.pt --source usb0 --resolution 1280x720
```
Here are all the arguments for yolo_detect.py:

- `--model`: Path to a model file (e.g. `my_model.pt`). If the model isn't found, it will default to using `yolov8s.pt`.
- `--source`: Source to run inference on. The options are:
    - Image file (example: `test.jpg`)
    - Folder of images (example: `my_images/test`)
    - Video file (example: `testvid.mp4`)
    - Index of a connected USB camera (example: `usb0`)
    - Index of a connected Picamera module for Raspberry Pi (example: `picamera0`)
- `--thresh` (optional): Minimum confidence threshold for displaying detected objects. Default value is 0.5 (example: `0.4`)
- `--resolution` (optional): Resolution in WxH to display inference results at. If not specified, the program will match the source resolution. (example: `1280x720`)
- `--record` (optional): Record a video of the results and save it as `demo1.avi`. (If using this option, the `--resolution` argument must also be specified.)


---

## ğŸ§© Training the Model

To start model training:
```bash
yolo detect train data=dataset/data.yaml model=yolov8n.pt --epochs 100 --mosaic 0.50 --optimizer AdamW --momentum 0.9
```
You can modify `--epochs` `--mosaic` `--optimizer` `--momentum` as per your choices

This will automatically create the following directory:
```
runs/detect/train/
 â”œâ”€â”€ weights/
 â”‚   â”œâ”€â”€ best.pt
 â”‚   â””â”€â”€ last.pt
 â”œâ”€â”€ results.png
 â”œâ”€â”€ confusion_matrix.png
 â””â”€â”€ metrics.csv
```

---

## ğŸ§ª Testing / Evaluating the Model

### Run Evaluation on Validation Set
```bash
yolo detect val model=runs/detect/train/weights/best.pt data=dataset/data.yaml
```

---

## ğŸ“Š Model Performance

| Metric | Value | Notes |
|:--------|:------:|:------|
| **mAP@0.5** |  | (Mean Average Precision at IoU threshold 0.5) |
| **Precision** |  | (Proportion of correct positive predictions) |
| **Recall** |  | (Proportion of actual positives correctly identified) |
| **Confusion Matrix** |  | (Matrix visualization of true vs predicted classes) |
| **Predictions (Sample)** |  | (Image samples with bounding boxes and labels) |



---

## ğŸ” Reproducing Final Results

To reproduce the same model results:
1. Clone the repo and set up the environment as per [Environment Setup](#ï¸-environment-setup).
2. Use the same dataset structure (`data.yaml`, train/val/test splits).
3. Train using the same configuration command.
4. Evaluate using the same validation command.
5. The results (metrics, weights, and logs) will be stored under `runs/detect/train/`.

---

## ğŸ–¼ï¸ Expected Outputs

### During Training:
- `results.png` â†’ shows training & validation loss, precision, recall, and mAP curves  
- `weights/best.pt` â†’ best model based on validation performance  
- `confusion_matrix.png` â†’ visual summary of class-wise predictions

### During Testing:
- Output images/videos with bounding boxes, class labels, and confidence scores  
- Example:
  ```
  detections/
   â”œâ”€â”€ img1_pred.jpg
   â”œâ”€â”€ img2_pred.jpg
  ```

Interpret the results as follows:
- **Bounding Box Color:** Represents detected class  
- **Confidence Score:** Modelâ€™s certainty about the detection  
- **Low Confidence (<0.4)** â†’ may indicate false positives or ambiguous cases  

---

## ğŸ§¾ Notes

- Modify hyperparameters in `config.yaml` or directly in the training command for optimization.
- Ensure the dataset is correctly annotated in YOLO format (one `.txt` file per image).
- Use GPU for faster training and inference (`torch.cuda.is_available()`).

---

---

## ğŸ“ Repository Structure
```
ğŸ“¦ Voltix
 â”£ ğŸ“‚ examples/
 â”£ ğŸ“‚ runs/
 â”£ ğŸ“œ classes.txt
 â”£ ğŸ“œ train.py
 â”£ ğŸ“œ predict.py
 â”£ ğŸ“œ visualize.py
 â”£ ğŸ“œ yolo_model.pt
 â”£ ğŸ“œ requirements.txt
 â”£ ğŸ“œ README.md
 â”— ğŸ“œ data.yaml
```
---
